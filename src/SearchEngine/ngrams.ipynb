{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Running:  (590, 10706)\n",
      "loading data\n",
      "testSchema:  (135060388, 158288947)\n"
     ]
    }
   ],
   "source": [
    "import tracemalloc\n",
    "from queryConstruction import constructQuery\n",
    "import globalVars\n",
    "from ranker import *\n",
    "from clustering import *\n",
    "from searchIndexer import *\n",
    "tracemalloc.start()\n",
    "print(\"Start Running: \",tracemalloc.get_traced_memory())\n",
    "\n",
    "print(\"loading data\")\n",
    "listOfQueries = getListQueries()\n",
    "path = globalVars.path\n",
    "\n",
    "##########load schema###########\n",
    "with open(path+'/TestSchemas/sportsSchema.pickle','rb') as file:\n",
    "    testSchema = pickle.load(file)\n",
    "    \n",
    "print(\"testSchema: \",tracemalloc.get_traced_memory())\n",
    "tracemalloc.stop()\n",
    "\n",
    "tracemalloc.start()\n",
    "###########one hot encoding############\n",
    "globalVars.init()\n",
    "\n",
    "OneHotVocab = globalVars.OneHotVocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosql\n",
      "greatSql\n",
      "nvBench\n",
      "sparc\n",
      "splash\n",
      "topics\n",
      "23\n",
      "127414\n",
      "78849\n",
      "{'query': 'select distinct farealias0.fare_id from airport_service as airport_servicealias0 , airport_service as airport_servicealias1 , city as cityalias0 , city as cityalias1 , days as daysalias0 , days as daysalias1 , days as daysalias2 , days as daysalias3 , days as daysalias4 , days as daysalias5 , days as daysalias6 , days as daysalias7 , days as daysalias8 , days as daysalias9 , fare as farealias0 , fare_basis as fare_basisalias0 , fare_basis as fare_basisalias1 , fare_basis as fare_basisalias2 , fare_basis as fare_basisalias3 , fare_basis as fare_basisalias4 , fare_basis as fare_basisalias5 , flight as flightalias0 , flight_fare as flight_farealias0 where ( ( ( ( ( ( ( ( ( ( daysalias8.day_name = \" day_name0 \" and daysalias9.day_name = \" day_name1 \" and flightalias0.flight_days = daysalias8.days_code and flightalias0.flight_days = daysalias9.days_code ) and daysalias7.day_name = \" day_name2 \" and flightalias0.flight_days = daysalias7.days_code ) and daysalias6.day_name = \" day_name3 \" and flightalias0.flight_days = daysalias6.days_code ) and daysalias5.day_name = \" day_name4 \" and flightalias0.flight_days = daysalias5.days_code ) and cityalias1.city_code = airport_servicealias1.city_code and cityalias1.city_name = \" city_name0 \" and flightalias0.to_airport = airport_servicealias1.airport_code ) and cityalias0.city_code = airport_servicealias0.city_code and cityalias0.city_name = \" city_name1 \" and daysalias4.day_name = \" day_name1 \" and fare_basisalias5.basis_days = daysalias4.days_code and farealias0.fare_basis_code = fare_basisalias5.fare_basis_code and flight_farealias0.fare_id = farealias0.fare_id and flightalias0.flight_id = flight_farealias0.flight_id and flightalias0.from_airport = airport_servicealias0.airport_code ) and daysalias3.day_name = \" day_name0 \" and fare_basisalias4.basis_days = daysalias3.days_code and farealias0.fare_basis_code = fare_basisalias4.fare_basis_code ) and daysalias2.day_name = \" day_name2 \" and fare_basisalias3.basis_days = daysalias2.days_code and farealias0.fare_basis_code = fare_basisalias3.fare_basis_code ) and daysalias1.day_name = \" day_name3 \" and fare_basisalias2.basis_days = daysalias1.days_code and farealias0.fare_basis_code = fare_basisalias2.fare_basis_code ) and daysalias0.day_name = \" day_name4 \" and fare_basisalias1.basis_days = daysalias0.days_code and farealias0.fare_basis_code = fare_basisalias1.fare_basis_code ) and fare_basisalias0.class_type = \" class_type0 \" and farealias0.fare_basis_code = fare_basisalias0.fare_basis_code ;', 'entities': ['airport_service', 'airport_service', 'city', 'city', 'days', 'days', 'days', 'days', 'days', 'days', 'days', 'days', 'days', 'days', 'fare', 'fare_basis', 'fare_basis', 'fare_basis', 'fare_basis', 'fare_basis', 'fare_basis', 'flight', 'flight_fare'], 'selectAttrs': ['fare.fare_id'], 'joinAttrs': [], 'groupByAttrs': [], 'orderByAttrs': [], 'aggrAttrs': [], 'whereAttrs': ['days.day_name', 'days.day_name', 'flight.flight_days', 'days.days_code', 'flight.flight_days', 'days.days_code', 'days.day_name', 'flight.flight_days', 'days.days_code', 'days.day_name', 'flight.flight_days', 'days.days_code', 'days.day_name', 'flight.flight_days', 'days.days_code', 'city.city_code', 'service.airport_city_code', 'city.city_name', 'flight.to_airport', 'service.airport_airport_code', 'city.city_code', 'service.airport_city_code', 'city.city_name', 'days.day_name', 'basis.fare_basis_days', 'days.days_code', 'fare.fare_basis_code', 'basis.fare_fare_basis_code', 'fare.flight_fare_id', 'fare.fare_id', 'flight.flight_id', 'fare.flight_flight_id', 'flight.from_airport', 'service.airport_airport_code', 'days.day_name', 'basis.fare_basis_days', 'days.days_code', 'fare.fare_basis_code', 'basis.fare_fare_basis_code', 'days.day_name', 'basis.fare_basis_days', 'days.days_code', 'fare.fare_basis_code', 'basis.fare_fare_basis_code', 'days.day_name', 'basis.fare_basis_days', 'days.days_code', 'fare.fare_basis_code', 'basis.fare_fare_basis_code', 'days.day_name', 'basis.fare_basis_days', 'days.days_code', 'fare.fare_basis_code', 'basis.fare_fare_basis_code', 'basis.fare_class_type', 'fare.fare_basis_code', 'basis.fare_fare_basis_code']}\n"
     ]
    }
   ],
   "source": [
    "#####get max number of entities in dataset and get total number of unique attributes ###\n",
    "import json\n",
    "datasets = [\"cosql\",\"greatSql\",\"nvBench\",\"sparc\",\"splash\",\"topics\"]\n",
    "maxEntities = 0\n",
    "totalSelectAttrs = 0\n",
    "totalWhereAttrs = 0\n",
    "d = \"\"\n",
    "l = {}\n",
    "for dataset in datasets:\n",
    "    with open('/home/nihal/Desktop/new_repo/GP/notebooks/preparingDatasets/finalOutputs/'+dataset+'.json', 'r',encoding='UTF-8') as file:\n",
    "        print(dataset)\n",
    "        data = json.load(file)\n",
    "        for query in data:\n",
    "            for q in query[\"allQueries\"]:\n",
    "                totalSelectAttrs = totalSelectAttrs + len(list(set(q[\"selectAttrs\"])))\n",
    "                totalWhereAttrs = totalWhereAttrs + len(list(set(q[\"whereAttrs\"])))\n",
    "                if len(q[\"entities\"]) > maxEntities:\n",
    "                    maxEntities = len(q[\"entities\"])\n",
    "                    l = q\n",
    "\n",
    "\n",
    "print(maxEntities)\n",
    "print(totalSelectAttrs)\n",
    "print(totalWhereAttrs)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-gram => prob of selecting attribute in general\n",
    "# 2-gram => prob of selecting attribute given entity\n",
    "# 3-gram => prob of selecting attribute given 2 entities \n",
    "# and so on .....\n",
    "# max number of entities 12 => 13-gram \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosql\n",
      "greatSql\n",
      "nvBench\n",
      "sparc\n",
      "splash\n",
      "topics\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/nihal/Desktop/new_repo/GP/src/SearchEngine/ngrams.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nihal/Desktop/new_repo/GP/src/SearchEngine/ngrams.ipynb#ch0000003?line=30'>31</a>\u001b[0m                         ngramsDict[\u001b[39m\"\u001b[39m\u001b[39mwhereAtrrsDict\u001b[39m\u001b[39m\"\u001b[39m][attrOneHotVector] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nihal/Desktop/new_repo/GP/src/SearchEngine/ngrams.ipynb#ch0000003?line=33'>34</a>\u001b[0m outputFile \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m/home/nihal/Desktop/new_repo/GP/src/SearchEngine/nGrams/ngrams.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/nihal/Desktop/new_repo/GP/src/SearchEngine/ngrams.ipynb#ch0000003?line=34'>35</a>\u001b[0m json\u001b[39m.\u001b[39;49mdump(ngramsDict, outputFile)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/nihal/Desktop/new_repo/GP/src/SearchEngine/ngrams.ipynb#ch0000003?line=35'>36</a>\u001b[0m outputFile\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/__init__.py?line=172'>173</a>\u001b[0m     iterable \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(skipkeys\u001b[39m=\u001b[39mskipkeys, ensure_ascii\u001b[39m=\u001b[39mensure_ascii,\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/__init__.py?line=173'>174</a>\u001b[0m         check_circular\u001b[39m=\u001b[39mcheck_circular, allow_nan\u001b[39m=\u001b[39mallow_nan, indent\u001b[39m=\u001b[39mindent,\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/__init__.py?line=174'>175</a>\u001b[0m         separators\u001b[39m=\u001b[39mseparators,\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/__init__.py?line=175'>176</a>\u001b[0m         default\u001b[39m=\u001b[39mdefault, sort_keys\u001b[39m=\u001b[39msort_keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\u001b[39m.\u001b[39miterencode(obj)\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/__init__.py?line=176'>177</a>\u001b[0m \u001b[39m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/__init__.py?line=177'>178</a>\u001b[0m \u001b[39m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/json/__init__.py?line=178'>179</a>\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/__init__.py?line=179'>180</a>\u001b[0m     fp\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=428'>429</a>\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=429'>430</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/json/encoder.py?line=430'>431</a>\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=431'>432</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=432'>433</a>\u001b[0m     \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=402'>403</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=403'>404</a>\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/json/encoder.py?line=404'>405</a>\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=405'>406</a>\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=406'>407</a>\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/encoder.py:376\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=373'>374</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=374'>375</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/lib/python3.8/json/encoder.py?line=375'>376</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mkeys must be str, int, float, bool or None, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=376'>377</a>\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=377'>378</a>\u001b[0m \u001b[39mif\u001b[39;00m first:\n\u001b[1;32m    <a href='file:///usr/lib/python3.8/json/encoder.py?line=378'>379</a>\u001b[0m     first \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: keys must be str, int, float, bool or None, not bytes"
     ]
    }
   ],
   "source": [
    "#####get unigram (calculating frequencies) for attributes ###\n",
    "import json\n",
    "datasets = [\"cosql\",\"greatSql\",\"nvBench\",\"sparc\",\"splash\",\"topics\"]\n",
    "ngramsDict = {}\n",
    "ngramsDict[\"selectAttrsDict\"] = {}\n",
    "ngramsDict[\"whereAtrrsDict\"] = {}\n",
    "for dataset in datasets:\n",
    "    with open('/home/nihal/Desktop/GP/finalOutputs/'+dataset+'.json', 'r',encoding='UTF-8') as file:\n",
    "        print(dataset)\n",
    "        data = json.load(file)\n",
    "        for query in data:\n",
    "            for q in query[\"allQueries\"]:\n",
    "                for attr in q[\"selectAttrs\"]:\n",
    "                    if attr.find(\".\") != -1:\n",
    "                        attr = attr.split(\".\")[1]\n",
    "                    attr = attr.split(\"_\")\n",
    "                    attrOneHotVector = (getKeyWordsVector(attr)).tostring()\n",
    "                    if ngramsDict[\"selectAttrsDict\"].get(attrOneHotVector) is None:\n",
    "                        ngramsDict[\"selectAttrsDict\"][attrOneHotVector] = 1\n",
    "                    else:\n",
    "                        ngramsDict[\"selectAttrsDict\"][attrOneHotVector] += 1\n",
    "                for attr in q[\"whereAttrs\"]:\n",
    "                    if attr.find(\".\") != -1:\n",
    "                        attr = attr.split(\".\")[1]\n",
    "                    attr = attr.split(\"_\")\n",
    "                    attrOneHotVector = (getKeyWordsVector(attr)).tostring()\n",
    "                    if ngramsDict[\"whereAtrrsDict\"].get(attrOneHotVector) is None:\n",
    "                        ngramsDict[\"whereAtrrsDict\"][attrOneHotVector] = 1\n",
    "                    else:\n",
    "                        ngramsDict[\"whereAtrrsDict\"][attrOneHotVector] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/home/nihal/Desktop/new_repo/GP/src/SearchEngine/nGrams/0.pickle\", 'wb') as handle:\n",
    "    pickle.dump(ngramsDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createNgramsDict(attributes,name,combinations):\n",
    "    for combination in combinations:\n",
    "        for attr in list(set(attributes)):\n",
    "            if attr.find(\".\") != -1:\n",
    "                attr = attr.split(\".\")[1]\n",
    "            attr = attr.split(\"_\")\n",
    "            attrOneHotVector = (getKeyWordsVector(attr)).tostring()\n",
    "            if nGramsDict[name][np.array(list(combination)).tostring()].get(attrOneHotVector) is None:\n",
    "                nGramsDict[name][np.array(list(combination)).tostring()][attrOneHotVector] = 0\n",
    "            nGramsDict[name][np.array(list(combination)).tostring()][attrOneHotVector] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosql\n",
      "greatSql\n",
      "nvBench\n",
      "sparc\n",
      "splash\n",
      "topics\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "datasets = [\"cosql\",\"greatSql\",\"nvBench\",\"sparc\",\"splash\",\"topics\"]\n",
    "nGramsDict = {}\n",
    "nGramsDict[\"selectAttrsDict\"] = {}\n",
    "nGramsDict[\"whereAtrrsDict\"] = {}\n",
    "for dataset in datasets:\n",
    "    with open('/home/nihal/Desktop/GP/finalOutputs/'+dataset+'.json', 'r',encoding='UTF-8') as file:\n",
    "        print(dataset)\n",
    "        data = json.load(file)\n",
    "        for query in data:\n",
    "            for q in query[\"allQueries\"]:\n",
    "                entitiesOneHotVector = []\n",
    "                combinations = []\n",
    "                for entity in list(set(q[\"entities\"])):\n",
    "                    entity = entity.split(\"_\")\n",
    "                    entityOneHotVector = (getKeyWordsVector(entity)).tostring()\n",
    "                    entitiesOneHotVector.append(entityOneHotVector)\n",
    "                for r in range(len(entitiesOneHotVector)+1):\n",
    "                    for combination in itertools.combinations(entitiesOneHotVector, r):\n",
    "                        if len(list(combination)) > 0:\n",
    "                            combinations.append(list(combination))\n",
    "                            nGramsDict[\"selectAttrsDict\"][np.array(list(combination)).tostring()] = {}\n",
    "                            nGramsDict[\"whereAtrrsDict\"][np.array(list(combination)).tostring()] = {}\n",
    "                \n",
    "                createNgramsDict(q[\"selectAttrs\"],\"selectAttrsDict\",combinations)\n",
    "                createNgramsDict(q[\"whereAttrs\"],\"whereAtrrsDict\",combinations)\n",
    "                        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/home/nihal/Desktop/new_repo/GP/src/SearchEngine/nGrams/allNGrams.pickle\", 'wb') as handle:\n",
    "    pickle.dump(nGramsDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
