{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Final Code starting from here #########################################\n",
    "from nltk.corpus import wordnet\n",
    "def getSynonyms(token):\n",
    "    synonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(token):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teacher', 'instructor']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSynonyms(\"instructor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nada/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    tokenization = word_tokenize(text)\n",
    "    lemma = wordnet_lemmatizer.lemmatize(tokenization[0])\n",
    "    return lemma\n",
    "def camel_case_paskal_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "def cleanColName(colName):\n",
    "    #seperate pascal and camel cases\n",
    "    colNames = camel_case_paskal_split(colName)\n",
    "    newColNames=[]\n",
    "    for name in colNames:\n",
    "      #lower\n",
    "      name = name.lower()\n",
    "      name = get_lemma(name)\n",
    "      newColNames.append(name)\n",
    "    return newColNames\n",
    "def cleanWord(token):\n",
    "  token = token.strip()\n",
    "  entityAlias=''\n",
    "  splittedTokens = []\n",
    "  ## clean entities if existed\n",
    "  if token.find(\".\") != -1:\n",
    "    splittedTokens = token.split(\".\")\n",
    "    # first element is the entity, second element is the attribute\n",
    "    entityTokens = re.split('_| |-',splittedTokens[0])\n",
    "    entities_clean = []\n",
    "    for token in entityTokens:\n",
    "      cleanedToken = cleanColName(token)\n",
    "      entities_clean.extend(cleanedToken)\n",
    "    entityAlias = \"_\".join(entities_clean) + '.'\n",
    "    token = splittedTokens[1]\n",
    "  ## clean attributes\n",
    "  attributes_clean=[]\n",
    "  attributeTokens = re.split('_| |-',token)\n",
    "  for token in attributeTokens:\n",
    "    cleanedToken = cleanColName(token)\n",
    "    attributes_clean.extend(cleanedToken)\n",
    "  finalCleanedAttrs = \"_\".join(attributes_clean)\n",
    "  return entityAlias+finalCleanedAttrs\n",
    "#################################################################\n",
    "def cleanTokens(tokens):\n",
    "  cleaned_tokens = []\n",
    "  for token in tokens:\n",
    "    cleaned_tokens.append(cleanWord(token))\n",
    "  return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "member_attendence.member_id\n",
      "manager\n"
     ]
    }
   ],
   "source": [
    "print(cleanWord(\"members_attendence.member_id\")) ############## problem exist here\n",
    "print(cleanWord(\"managers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_queries(query):\n",
    "    queries=[]\n",
    "    runing_query=[]\n",
    "    i = 1\n",
    "    runing_query.append(query[0])\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "\n",
    "        if token == 'select':\n",
    "            prev_token = query[i-1]\n",
    "            nested_query = []\n",
    "            if prev_token == '(':\n",
    "                #WHILE TO GET BRACKET END\n",
    "                open_bracket = 1\n",
    "                while i<len(query) and open_bracket >=1:\n",
    "                    token = query[i]\n",
    "                    i+=1\n",
    "                    if token == '(': open_bracket += 1\n",
    "                    elif token == ')': open_bracket -= 1\n",
    "                    if open_bracket == 0: break\n",
    "                    nested_query.append(token)\n",
    "            else:\n",
    "                nested_query = query[i:]\n",
    "                i = len(query)+1\n",
    "            nested_queries = separate_queries(nested_query)\n",
    "            queries.extend(nested_queries)\n",
    "        if i <= len(query): runing_query.append(token)\n",
    "        i+=1\n",
    "    queries.append(runing_query)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntitiesAliasesMapping(query):\n",
    "    entities_aliases = {}\n",
    "    i = 0\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "        if token == 'as':\n",
    "            entity = query[i-1]\n",
    "            alias = query[i+1]\n",
    "            entities_aliases[alias] = entity\n",
    "        i+=1\n",
    "    return entities_aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixWhereClause(whereClauseInfo):\n",
    "    if len(whereClauseInfo) == 2:\n",
    "        whereClauseInfo.append(\"value\")\n",
    "    if len(whereClauseInfo) == 3 and whereClauseInfo[2] in [\"and\",\"or\"]:\n",
    "        tempCondition = whereClauseInfo.pop()\n",
    "        whereClauseInfo.append(\"value\")\n",
    "        whereClauseInfo.append(tempCondition)\n",
    "    if len(whereClauseInfo) == 3:\n",
    "        whereClauseInfo.append(\"None\")\n",
    "    return whereClauseInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### my getqueries function ###\n",
    "################# new function #######################\n",
    "########### changing where clause list #################\n",
    "stop_words = [\"desc\",\"asc\"]\n",
    "AggrFunctions = [\"count\",\"sum\",\"min\",\"max\",\"avg\"]\n",
    "tokensToSkip = [\",\",\"(\",\")\",\".\",\"by\",\"distinct\",\";\",\"-\",\"null\"]\n",
    "endOfWhere = [\"and\",\"or\",\"in\",\"not\",\"union\",\"intersect\",\"except\",\"like\"]\n",
    "whereClauseEnd = [\"group\",\"order\",\"limit\",\"offset\",\"having\"]\n",
    "def get_queries(query,realQuery):\n",
    "    entities = []\n",
    "    selectAttrs = []\n",
    "    joinAttrs = []\n",
    "    groupByAttrs = []\n",
    "    orderByAttrs = []\n",
    "    aggrAttrs = []\n",
    "    whereAttrs = []\n",
    "    havingAttrs =[]\n",
    "    aliases = getEntitiesAliasesMapping(query)\n",
    "    i = 0\n",
    "    token = \"\"\n",
    "    wordType = \"\"\n",
    "    orderFunction = \"\"\n",
    "    aggrType = \"\"\n",
    "    whereClauseInfo = []\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "        if token == \"select\":\n",
    "            wordType = \"select\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token in stop_words:\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"limit\":\n",
    "            i += 2\n",
    "            continue\n",
    "        if (token[0] == \"'\" or token[0] == '\"') and (token[len(token)-1] == \"'\" or token[len(token)-1] == '\"') and len(token) != 1:\n",
    "            i+=1\n",
    "            continue\n",
    "        if token.find('\\\"') != -1 or token.find(\"'\") != -1 or token.find('\"') != -1 or token.find(\"%\") != -1:\n",
    "            #find the next same token\n",
    "            i+=1\n",
    "            while i < len(query) and query[i]!=token:\n",
    "                i+=1\n",
    "            i += 1\n",
    "            continue\n",
    "        if token in tokensToSkip:\n",
    "            if token == \")\" and wordType == \"aggr\":\n",
    "                wordType = \"select\"\n",
    "            if token == \"(\" and wordType in [\"whereClause\",\"havingClause\"] and query[i-1]==\"in\":\n",
    "                open_bracket = 1\n",
    "                i+=1\n",
    "                while i<len(query) and open_bracket >=1:\n",
    "                    token = query[i]\n",
    "                    if token == '(': open_bracket += 1\n",
    "                    elif token == ')': open_bracket -= 1\n",
    "                    if open_bracket == 0: break\n",
    "                    i+=1\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == 'as':\n",
    "            i+=2\n",
    "            continue \n",
    "        if token == \"group\":\n",
    "            wordType = \"groupBy\"\n",
    "            i += 2\n",
    "            continue\n",
    "        if token == \"order\":\n",
    "            wordType = \"orderBy\"\n",
    "            if query[i+2] in AggrFunctions:\n",
    "                orderFunction = query[i+2]\n",
    "                i += 3\n",
    "            else: \n",
    "                orderFunction = \"\"\n",
    "                i += 2\n",
    "            continue\n",
    "        if token == \"having\":\n",
    "            wordType = \"havingClause\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token in AggrFunctions and wordType==\"havingClause\":\n",
    "            whereClauseInfo.append(token)\n",
    "            i += 1\n",
    "            continue    \n",
    "        if token in AggrFunctions and wordType!=\"havingClause\":\n",
    "            wordType = \"aggr\"\n",
    "            aggrType = token\n",
    "            i+= 1\n",
    "            continue\n",
    "        if token == \"from\" or token == \"join\":\n",
    "            wordType = \"entity\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"where\":\n",
    "            wordType = \"whereClause\"\n",
    "            i += 1\n",
    "            continue     \n",
    "        if token == \"on\":\n",
    "            wordType = \"join\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if len(whereClauseInfo)==2 and token.find(\"var\")!=-1:\n",
    "            i += 1\n",
    "            continue\n",
    "        # check for numbers to skip\n",
    "        if re.match(r'[0-9]+',token):\n",
    "            i += 1\n",
    "            continue\n",
    "        if wordType in [\"select\",\"groupBy\",\"orderBy\",\"whereClause\",\"join\",\"aggr\",\"havingClause\"]:\n",
    "            if i+1 < len(query) and query[i+1] == \".\":\n",
    "                token = aliases.get(token,token)\n",
    "                token = token + \".\"+query[i+2]\n",
    "                i += 2\n",
    "        if wordType in [\"whereClause\",\"havingClause\"] :\n",
    "            if token not in endOfWhere:\n",
    "                if token in [\"!\",\">\",\"<\"] and query[i+1] == \"=\":\n",
    "                    token += query[i+1]\n",
    "                    i += 1\n",
    "                whereClauseInfo.append(token)\n",
    "                if i+1<len(query) and query[i+1] not in whereClauseEnd:       \n",
    "                    i += 1\n",
    "                    continue\n",
    "            if token == \"like\":\n",
    "                whereClauseInfo.append(token)\n",
    "                i += 1\n",
    "                continue\n",
    "            if token == \"not\" and query[i+1] in [\"in\",\"like\",\"is\"]:\n",
    "                token = token + ' ' + query[i+1]\n",
    "                whereClauseInfo.append(token)\n",
    "                i += 1\n",
    "            ### end of where condition (and , or , in)\n",
    "            if token in [\"and\",\"or\",\"in\"]:\n",
    "                if len(whereClauseInfo) > 0:\n",
    "                    whereClauseInfo.append(token)\n",
    "                else:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "        if token in [\"union\",\"intersect\",\"except\"]:\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"=\" and wordType != \"whereClause\":\n",
    "            i += 1\n",
    "            continue\n",
    "        token = cleanTokens([token])[0]\n",
    "        sepToken = token.split(\".\")[-1]\n",
    "        token_syns = list(set(getSynonyms(sepToken)))\n",
    "        token_syns = cleanTokens(token_syns)\n",
    "        synonyms_dict[sepToken] = token_syns\n",
    "        if wordType == \"select\":\n",
    "            selectAttrs.append(token)\n",
    "        elif wordType == \"entity\":\n",
    "            entities.append(token)\n",
    "        elif wordType == \"join\":\n",
    "            joinAttrs.append(token)\n",
    "        elif wordType == \"groupBy\":\n",
    "            groupByAttrs.append(token)\n",
    "        elif wordType == \"orderBy\":\n",
    "            orderByAttrs.append((token, orderFunction))\n",
    "        elif wordType == \"aggr\":\n",
    "            aggrAttrs.append((token,aggrType))\n",
    "            aggrType = \"\"\n",
    "        elif wordType == \"whereClause\":\n",
    "            whereClauseInfo = fixWhereClause(whereClauseInfo)\n",
    "            if len(whereClauseInfo) > 0:\n",
    "                whereAttrs.append(whereClauseInfo)\n",
    "                whereClauseInfo = []\n",
    "                if i+1 < len(query) and query[i+1].find(\"var\")!=-1:\n",
    "                    i += 1\n",
    "        elif wordType == \"havingClause\": \n",
    "            if len(whereClauseInfo) > 0:\n",
    "                havingAttrs.append(whereClauseInfo)\n",
    "                whereClauseInfo = []\n",
    "                if i+1 < len(query) and query[i+1].find(\"var\")!=-1:\n",
    "                    i += 1\n",
    "        i += 1\n",
    "    if len(whereClauseInfo) != 0:\n",
    "        if wordType == \"whereClause\":\n",
    "            whereClauseInfo = fixWhereClause(whereClauseInfo)\n",
    "            whereAttrs.append(whereClauseInfo)\n",
    "            whereClauseInfo = []\n",
    "        elif wordType == \"havingClause\":\n",
    "            havingAttrs.append(whereClauseInfo)\n",
    "            whereClauseInfo = []\n",
    "    if len(whereAttrs) > 0:\n",
    "        if len(whereAttrs[-1]) == 4:\n",
    "            if whereAttrs[-1][3] in [\"and\",\"or\"]:\n",
    "                whereAttrs[-1][3] = \"None\"\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = realQuery\n",
    "    tempDict[\"entities\"] = list(entities)\n",
    "    tempDict[\"selectAttrs\"] = list(selectAttrs)\n",
    "    tempDict[\"joinAttrs\"] = list(joinAttrs)\n",
    "    tempDict[\"groupByAttrs\"] = list(groupByAttrs)\n",
    "    tempDict[\"orderByAttrs\"] = list(orderByAttrs)\n",
    "    tempDict[\"aggrAttrs\"] = list(aggrAttrs)\n",
    "    tempDict[\"whereAttrs\"] = list(whereAttrs)\n",
    "    tempDict[\"havingAttrs\"] = list(havingAttrs)\n",
    "    return tempDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': ['select', '*', 'from', 'questions', 'where', 'questions', '.', 'body', 'not', 'like', \"'\", '%', 'multivariate', 'analysis', '%', \"'\"], 'entities': ['question'], 'selectAttrs': ['*'], 'joinAttrs': [], 'groupByAttrs': [], 'orderByAttrs': [], 'aggrAttrs': [], 'whereAttrs': [['questions.body', 'not like', 'value', 'None']], 'havingAttrs': []}\n"
     ]
    }
   ],
   "source": [
    "synonyms_dict = {}\n",
    "\n",
    "query_toks =  [\"select\", \"*\", \"from\", \"questions\", \"where\", \"questions\", \".\", \"body\", \"not\", \"like\", \"'\", \"%\", \"multivariate\", \"analysis\", \"%\", \"'\"]\n",
    "\n",
    "nested = separate_queries(query_toks)\n",
    "for query in nested:\n",
    "    print(get_queries(query,query_toks))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8919\n",
      "45942\n",
      "7247\n",
      "10228\n",
      "9493\n",
      "5183\n",
      "189\n",
      "214\n",
      "259\n",
      "99\n",
      "23\n",
      "284\n",
      "5183\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "############# this cell for datasets:  sparc and splash-master and cosql ###########\n",
    "import json\n",
    "import re\n",
    "synonyms_dict = {}\n",
    "datasets = [\"cosql\",\"greatSql\",\"nvBench\",\"sparc\",\"splash\",\"spider\",\"academic\",\"advising\",\"geography\",\"imdb\",\"restaurants\",\"scholar\",\"spider\",\"yelp\"]\n",
    "for ds in datasets:\n",
    "    keywordsDict = []\n",
    "    with open('/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/'+ds+'.json', 'r',encoding='UTF-8') as file:\n",
    "        data = json.load(file)\n",
    "        for query in data:\n",
    "            nestedQueries = []\n",
    "            query_tokens = query['query_toks']\n",
    "            separatedQueries = separate_queries(query_tokens)  \n",
    "            for currentQuery in separatedQueries:\n",
    "                concatQuery = ' '.join(currentQuery)\n",
    "                concatQuery = concatQuery.replace(\" . \",\".\")\n",
    "                tempDict = get_queries(currentQuery,concatQuery)\n",
    "                nestedQueries.append(tempDict)\n",
    "            outQuery = {\n",
    "                'query':query['query'],\n",
    "                'allQueries':nestedQueries\n",
    "            }\n",
    "            keywordsDict.append(outQuery)\n",
    "    print(len(keywordsDict))\n",
    "    outputFile = open(\"./finalOutputs/\"+ds+\".json\", \"w\")\n",
    "    json.dump(keywordsDict, outputFile)\n",
    "    outputFile.close()\n",
    "outputFile = open(\"./finalOutputs/synonyms.json\", \"w\")\n",
    "json.dump(synonyms_dict, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### splash-master data tokenization #####\n",
    "import re\n",
    "import json\n",
    "Dataset = []\n",
    "def read_data():\n",
    "    data = []\n",
    "    data_path = \"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/Splash-master/data\"\n",
    "    data_files = [data_path + '/' + f for f in os.listdir(data_path) if f.endswith('.json')]\n",
    "    for data_file in data_files:\n",
    "        with open(data_file) as f:\n",
    "            data_object = json.load(f)\n",
    "            queries = [data['gold_parse'] for data in data_object]\n",
    "            data.extend(queries)\n",
    "    return data\n",
    "data = read_data()\n",
    "\n",
    "for query in data:\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = query\n",
    "    tokensList = re.split('(\\W)', query)\n",
    "    tokensList = list(filter(None, tokensList))\n",
    "    tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "    convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "    tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "    Dataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/splash.json\", \"w\")\n",
    "json.dump(Dataset, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### sparc data tokenization #####\n",
    "import re\n",
    "import json\n",
    "Dataset = []\n",
    "def read_data():\n",
    "    data = []\n",
    "    data_path = \"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/sparc/sparc\"\n",
    "    data_files = [data_path + '/' + f for f in os.listdir(data_path) if f.endswith('.json')]\n",
    "    for data_file in data_files:\n",
    "        with open(data_file) as f:\n",
    "            data_object = json.load(f)\n",
    "            interactions = [data['interaction'] for data in data_object]\n",
    "            for interaction  in interactions:\n",
    "                for query in interaction:\n",
    "                    data.append(query['query'])\n",
    "    return data\n",
    "data = read_data()\n",
    "for query in data:\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = query\n",
    "    tokensList = re.split('(\\W)', query)\n",
    "    tokensList = list(filter(None, tokensList))\n",
    "    tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "    convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "    tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "    Dataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/sparc.json\", \"w\")\n",
    "json.dump(Dataset, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### nvBench data tokenization #####\n",
    "import re\n",
    "import json\n",
    "Dataset = []\n",
    "def read_data():\n",
    "    data = []\n",
    "    with open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/nvBench.json\") as f:\n",
    "        data_object = json.load(f)\n",
    "        queries = [data['vis_query']['data_part']['sql_part'] for data in data_object.values()]\n",
    "        data.extend(queries)\n",
    "    return data\n",
    "data = read_data()\n",
    "for query in data:\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = query\n",
    "    tokensList = re.split('(\\W)', query)\n",
    "    tokensList = list(filter(None, tokensList))\n",
    "    tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "    convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "    tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "    Dataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/nvBench.json\", \"w\")\n",
    "json.dump(Dataset, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cosql data tokenization #####\n",
    "import re\n",
    "import json\n",
    "Dataset = []\n",
    "def read_data():\n",
    "    data=[]\n",
    "    data_path = \"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/cosql_dataset/cosql_dataset/system_response_generation\"\n",
    "    data_files = [data_path + '/' + f for f in os.listdir(data_path) if f.endswith('.json')]\n",
    "    for data_file in data_files:\n",
    "        with open(data_file) as f:\n",
    "            data_object = json.load(f)\n",
    "            queries = [{\"query\":data['query'],\"query_toks\":data[\"query_toks\"]}for data in data_object]\n",
    "            data.extend(queries)\n",
    "    return data \n",
    "data = read_data()\n",
    "for query in data:\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = query[\"query\"]\n",
    "    tempDict[\"query_toks\"] = query['query_toks']\n",
    "    Dataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/cosql.json\", \"w\")\n",
    "json.dump(Dataset, outputFile)\n",
    "outputFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Topics data tokenization #####\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "datapath =\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/topics/\"\n",
    "data_files = os.listdir(datapath) \n",
    "for data_file in data_files:\n",
    "    Dataset = []\n",
    "    def read_data(datapth):\n",
    "        data=[]\n",
    "        with open(datapth) as f:\n",
    "            data_object = json.load(f)\n",
    "            queries = [data['sql'] for data in data_object]\n",
    "            for q in queries:\n",
    "                data.extend(q)\n",
    "        return data\n",
    "    data = read_data(datapath+data_file)\n",
    "    for query in data:\n",
    "        tempDict = {}\n",
    "        tempDict[\"query\"] = query\n",
    "        tokensList = re.split('(\\W)', query)\n",
    "        tokensList = list(filter(None, tokensList))\n",
    "        tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "        convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "        tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "        Dataset.append(tempDict)\n",
    "\n",
    "    outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/\"+data_file, \"w\")\n",
    "    json.dump(Dataset, outputFile)\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GreatSql data tokenization #####\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "Dataset = []\n",
    "def read_data():\n",
    "    data=[]\n",
    "    datapath =\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/GreatSQL-master/data\"\n",
    "    data_files = os.listdir(datapath) \n",
    "    for data_file in data_files:\n",
    "        with open(datapath+\"/\"+data_file) as f:\n",
    "            data_object = json.load(f)\n",
    "            queries = [data['sql'] for data in data_object]\n",
    "            data.extend(queries)\n",
    "    return data\n",
    "data = read_data()\n",
    "for query in data:\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = query\n",
    "    tokensList = re.split('(\\W)', query)\n",
    "    tokensList = list(filter(None, tokensList))\n",
    "    tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "    convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "    tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "    Dataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/greatSql.json\", \"w\")\n",
    "json.dump(Dataset, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# old function #######################\n",
    "### my getqueries function ###\n",
    "stop_words = ['in','intersect','except','union','not',\"desc\",\"asc\",\"or\",\"and\",\"like\"]\n",
    "AggrFunctions = [\"count\",\"sum\",\"min\",\"max\",\"avg\"]\n",
    "tokensToSkip = [\",\",\"(\",\")\",\".\",\"=\",\"!\",\"by\",\"distinct\",\">\",\">=\",\"<\",\"<=\",\";\"]\n",
    "def get_queries(query,realQuery):\n",
    "    entities = []\n",
    "    selectAttrs = []\n",
    "    joinAttrs = []\n",
    "    groupByAttrs = []\n",
    "    orderByAttrs = []\n",
    "    aggrAttrs = []\n",
    "    whereAttrs = []\n",
    "    aliases = getEntitiesAliasesMapping(query)\n",
    "    i = 0\n",
    "    token = \"\"\n",
    "    wordType = \"\"\n",
    "    orderFunction = \"\"\n",
    "    whereCondition = \"\"\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "        if token == \"select\":\n",
    "            wordType = \"select\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token in stop_words:\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"limit\":\n",
    "            i += 2\n",
    "            continue\n",
    "        if token.find('\\\"') != -1 or token.find(\"'\") != -1 or token.find('\"') != -1:\n",
    "            i += 1\n",
    "            while i<len(query) and query[i].find('\\\"') == -1 and query[i].find(\"'\") == -1 and query[i].find('\"') == -1:\n",
    "                i += 1\n",
    "            i += 1\n",
    "            continue\n",
    "        if token.find(\"'\")!=-1 or token.find('\"')!=-1:\n",
    "            i+=1\n",
    "            continue\n",
    "        if token in tokensToSkip:\n",
    "            if token == \")\" and wordType == \"aggr\":\n",
    "                wordType = \"select\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == 'as':\n",
    "            i+=2\n",
    "            continue \n",
    "        if token == \"group\":\n",
    "            wordType = \"groupBy\"\n",
    "            i += 2\n",
    "            continue\n",
    "        if token == \"order\":\n",
    "            wordType = \"orderBy\"\n",
    "            if query[i+2] in AggrFunctions:\n",
    "                orderFunction = query[i+2]\n",
    "                i += 3\n",
    "            else: \n",
    "                orderFunction = \"\"\n",
    "                i += 2\n",
    "            continue\n",
    "        if token in AggrFunctions:\n",
    "            wordType = \"aggr\"\n",
    "            i+= 1\n",
    "            continue\n",
    "        if token == \"from\" or token == \"join\":\n",
    "            wordType = \"entity\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"where\":\n",
    "            wordType = \"whereClause\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"on\":\n",
    "            wordType = \"join\"\n",
    "            i += 1\n",
    "            continue\n",
    "        # check for numbers to skip\n",
    "        if re.match(r'[0-9]+',token):\n",
    "            i += 1\n",
    "            continue\n",
    "        if wordType in [\"select\",\"groupBy\",\"orderBy\",\"whereClause\",\"join\",\"aggr\"]:\n",
    "            if i+1 < len(query) and query[i+1] == \".\":\n",
    "                token = aliases.get(token,token)\n",
    "                token = token + \".\"+query[i+2]\n",
    "                i += 2\n",
    "            if wordType == \"whereClause\":\n",
    "                whereConditionsList = [\">\",\"<\",\"=\",\"!\"]\n",
    "                if i+1 < len(query):\n",
    "                    if query[i+1] in whereConditionsList:\n",
    "                        if i+2 < len(query) and query[i+2] in whereConditionsList:\n",
    "                            whereCondition = query[i+1] + query[i+2]\n",
    "                            i += 2\n",
    "                        else:\n",
    "                            whereCondition = query[i+1]\n",
    "                            i += 1\n",
    "            \n",
    "                \n",
    "        token = cleanTokens([token])[0]\n",
    "        sepToken = token.split(\".\")[-1]\n",
    "        token_syns = list(set(getSynonyms(sepToken)))\n",
    "        token_syns = cleanTokens(token_syns)\n",
    "        synonyms_dict[sepToken] = token_syns\n",
    "        if wordType == \"select\":\n",
    "            selectAttrs.append(token)\n",
    "        elif wordType == \"entity\":\n",
    "            entities.append(token)\n",
    "        elif wordType == \"join\":\n",
    "            joinAttrs.append(token)\n",
    "        elif wordType == \"groupBy\":\n",
    "            groupByAttrs.append(token)\n",
    "        elif wordType == \"orderBy\":\n",
    "            orderByAttrs.append((token, orderFunction))\n",
    "        elif wordType == \"aggr\":\n",
    "            aggrAttrs.append(token)\n",
    "        elif wordType == \"whereClause\":\n",
    "            whereAttrs.append((token,whereCondition))\n",
    "            whereCondition = \"\"\n",
    "        i += 1\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = realQuery\n",
    "    tempDict[\"entities\"] = list(entities)\n",
    "    tempDict[\"selectAttrs\"] = list(selectAttrs)\n",
    "    tempDict[\"joinAttrs\"] = list(joinAttrs)\n",
    "    tempDict[\"groupByAttrs\"] = list(groupByAttrs)\n",
    "    tempDict[\"orderByAttrs\"] = list(orderByAttrs)\n",
    "    tempDict[\"aggrAttrs\"] = list(aggrAttrs)\n",
    "    tempDict[\"whereAttrs\"] = list(whereAttrs)\n",
    "    return tempDict"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
