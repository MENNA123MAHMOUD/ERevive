{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Final Code starting from here #########################################\n",
    "from nltk.corpus import wordnet\n",
    "def getSynonyms(token):\n",
    "    synonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(token):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    return list(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['teacher', 'instructor']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSynonyms(\"instructor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_paskal_split(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "def cleanColName(colName):\n",
    "    #remove names with one character \n",
    "\n",
    "    #seperate pascal and camel cases\n",
    "    entity= ''\n",
    "    if colName.find('.')!=-1:\n",
    "        entity = colName.split('.')\n",
    "        colName = entity[1]   \n",
    "        entity = cleanOneToken(entity[0])+'.'\n",
    "    \n",
    "    colNames = camel_case_paskal_split(colName)\n",
    "    \n",
    "    newColNames=[]\n",
    "    for name in colNames:\n",
    "      #lower\n",
    "      name = name.lower()\n",
    "      newColNames.append(name)\n",
    "    \n",
    "    return newColNames,entity\n",
    "def cleanOneToken(token):\n",
    "    token = token.strip()\n",
    "    token = re.split('_| |-',token)\n",
    "    # print(token)\n",
    "    attributes_clean=[]\n",
    "    entityAlias=''\n",
    "    for att in token:\n",
    "      atts,entity = cleanColName(att)\n",
    "      if entity!='':\n",
    "        entityAlias=entity\n",
    "      attributes_clean.extend(atts)\n",
    "      # print(\"d\",attributes_clean)\n",
    "    attributes_clean = \"_\".join(attributes_clean)\n",
    "    # print(attributes_clean)\n",
    "    return entityAlias+attributes_clean\n",
    "def cleanTokens(tokens):\n",
    "  cleaned_tokens = []\n",
    "  for token in tokens:\n",
    "    cleaned_tokens.append(cleanOneToken(token))\n",
    "  return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['member', 'attendence.member', 'id']\n",
      "d ['member']\n",
      "['attendence']\n",
      "d ['attendence']\n",
      "attendence\n",
      "d ['member', 'member']\n",
      "d ['member', 'member', 'id']\n",
      "member_member_id\n",
      "attendence.member_member_id\n"
     ]
    }
   ],
   "source": [
    "print(cleanOneToken(\"member_attendence.member_id\")) ############## problem exist here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_queries(query):\n",
    "    queries=[]\n",
    "    runing_query=[]\n",
    "    i = 1\n",
    "    runing_query.append(query[0])\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "\n",
    "        if token == 'select':\n",
    "            prev_token = query[i-1]\n",
    "            nested_query = []\n",
    "            if prev_token == '(':\n",
    "                #WHILE TO GET BRACKET END\n",
    "                open_bracket = 1\n",
    "                while i<len(query) and open_bracket >=1:\n",
    "                    token = query[i]\n",
    "                    i+=1\n",
    "                    if token == '(': open_bracket += 1\n",
    "                    elif token == ')': open_bracket -= 1\n",
    "                    if open_bracket == 0: break\n",
    "                    nested_query.append(token)\n",
    "            else:\n",
    "                nested_query = query[i:]\n",
    "                i = len(query)+1\n",
    "            nested_queries = separate_queries(nested_query)\n",
    "            queries.extend(nested_queries)\n",
    "        if i <= len(query): runing_query.append(token)\n",
    "        i+=1\n",
    "    queries.append(runing_query)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntitiesAliasesMapping(query):\n",
    "    entities_aliases = {}\n",
    "    i = 0\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "        if token == 'as':\n",
    "            entity = query[i-1]\n",
    "            alias = query[i+1]\n",
    "            entities_aliases[alias] = entity\n",
    "        i+=1\n",
    "    return entities_aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixWhereClause(whereClauseInfo):\n",
    "    if len(whereClauseInfo) == 2:\n",
    "        whereClauseInfo.append(\"value\")\n",
    "    if len(whereClauseInfo) == 3 and whereClauseInfo[2] in [\"and\",\"or\"]:\n",
    "        tempCondition = whereClauseInfo.pop()\n",
    "        whereClauseInfo.append(\"value\")\n",
    "        whereClauseInfo.append(tempCondition)\n",
    "    if len(whereClauseInfo) == 3:\n",
    "        whereClauseInfo.append(\"None\")\n",
    "    return whereClauseInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### my getqueries function ###\n",
    "################# new function #######################\n",
    "########### changing where clause list #################\n",
    "stop_words = [\"desc\",\"asc\"]\n",
    "AggrFunctions = [\"count\",\"sum\",\"min\",\"max\",\"avg\"]\n",
    "tokensToSkip = [\",\",\"(\",\")\",\".\",\"by\",\"distinct\",\";\"]\n",
    "endOfWhere = [\"and\",\"or\",\"in\",\"not\",\"union\",\"intersect\",\"except\",\"like\"]\n",
    "def get_queries(query,realQuery):\n",
    "    entities = []\n",
    "    selectAttrs = []\n",
    "    joinAttrs = []\n",
    "    groupByAttrs = []\n",
    "    orderByAttrs = []\n",
    "    aggrAttrs = []\n",
    "    whereAttrs = []\n",
    "    aliases = getEntitiesAliasesMapping(query)\n",
    "    i = 0\n",
    "    token = \"\"\n",
    "    wordType = \"\"\n",
    "    orderFunction = \"\"\n",
    "    aggrType = \"\"\n",
    "    whereClauseInfo = []\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "        if token == \"select\":\n",
    "            wordType = \"select\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token in stop_words:\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"limit\":\n",
    "            i += 2\n",
    "            continue\n",
    "        if (token[0] == \"'\" or token[0] == '\"') and (token[len(token)-1] == \"'\" or token[len(token)-1] == '\"') and len(token) != 1:\n",
    "            i+=1\n",
    "            continue\n",
    "        if token.find('\\\"') != -1 or token.find(\"'\") != -1 or token.find('\"') != -1:\n",
    "            i += 1\n",
    "            while i<len(query) and query[i].find('\\\"') == -1 and query[i].find(\"'\") == -1 and query[i].find('\"') == -1:\n",
    "                i += 1\n",
    "            i += 1\n",
    "            continue\n",
    "        if token in tokensToSkip:\n",
    "            if token == \")\" and wordType == \"aggr\":\n",
    "                wordType = \"select\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == 'as':\n",
    "            i+=2\n",
    "            continue \n",
    "        if token == \"group\":\n",
    "            wordType = \"groupBy\"\n",
    "            i += 2\n",
    "            continue\n",
    "        if token == \"order\":\n",
    "            wordType = \"orderBy\"\n",
    "            if query[i+2] in AggrFunctions:\n",
    "                orderFunction = query[i+2]\n",
    "                i += 3\n",
    "            else: \n",
    "                orderFunction = \"\"\n",
    "                i += 2\n",
    "            continue\n",
    "        if token in AggrFunctions:\n",
    "            wordType = \"aggr\"\n",
    "            aggrType = token\n",
    "            i+= 1\n",
    "            continue\n",
    "        if token == \"from\" or token == \"join\":\n",
    "            wordType = \"entity\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"where\":\n",
    "            wordType = \"whereClause\"\n",
    "            i += 1\n",
    "            continue     \n",
    "        if token == \"on\":\n",
    "            wordType = \"join\"\n",
    "            i += 1\n",
    "            continue\n",
    "        # check for numbers to skip\n",
    "        if re.match(r'[0-9]+',token):\n",
    "            i += 1\n",
    "            continue\n",
    "        if wordType in [\"select\",\"groupBy\",\"orderBy\",\"whereClause\",\"join\",\"aggr\"]:\n",
    "            if i+1 < len(query) and query[i+1] == \".\":\n",
    "                token = aliases.get(token,token)\n",
    "                token = token + \".\"+query[i+2]\n",
    "                i += 2\n",
    "        if wordType == \"whereClause\":\n",
    "            if token not in endOfWhere:\n",
    "                if token in [\"!\",\">\",\"<\"] and query[i+1] == \"=\":\n",
    "                    token += query[i+1]\n",
    "                    i += 1\n",
    "                whereClauseInfo.append(token)\n",
    "                i += 1\n",
    "                continue\n",
    "            if token == \"like\":\n",
    "                whereClauseInfo.append(token)\n",
    "                i += 1\n",
    "                continue\n",
    "            if token == \"not\" and query[i+1] in [\"in\",\"like\"]:\n",
    "                token = token + ' ' + query[i+1]\n",
    "                whereClauseInfo.append(token)\n",
    "                i += 2\n",
    "            ### end of where condition (and , or , in)\n",
    "            if token in [\"and\",\"or\",\"in\"]:\n",
    "                whereClauseInfo.append(token)\n",
    "                \n",
    "        if token in [\"union\",\"intersect\",\"except\"]:\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"=\" and wordType != \"whereClause\":\n",
    "            i += 1\n",
    "            continue\n",
    "        token = cleanTokens([token])[0]\n",
    "        sepToken = token.split(\".\")[-1]\n",
    "        token_syns = list(set(getSynonyms(sepToken)))\n",
    "        token_syns = cleanTokens(token_syns)\n",
    "        synonyms_dict[sepToken] = token_syns\n",
    "        if wordType == \"select\":\n",
    "            selectAttrs.append(token)\n",
    "        elif wordType == \"entity\":\n",
    "            entities.append(token)\n",
    "        elif wordType == \"join\":\n",
    "            joinAttrs.append(token)\n",
    "        elif wordType == \"groupBy\":\n",
    "            groupByAttrs.append(token)\n",
    "        elif wordType == \"orderBy\":\n",
    "            orderByAttrs.append((token, orderFunction))\n",
    "        elif wordType == \"aggr\":\n",
    "            aggrAttrs.append((token,aggrType))\n",
    "            aggrType = \"\"\n",
    "        elif wordType == \"whereClause\":\n",
    "            whereClauseInfo = fixWhereClause(whereClauseInfo)\n",
    "            whereAttrs.append(whereClauseInfo)\n",
    "            whereClauseInfo = []\n",
    "\n",
    "        i += 1\n",
    "    if len(whereClauseInfo) != 0:\n",
    "        whereClauseInfo = fixWhereClause(whereClauseInfo)\n",
    "        whereAttrs.append(whereClauseInfo)\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = realQuery\n",
    "    tempDict[\"entities\"] = list(entities)\n",
    "    tempDict[\"selectAttrs\"] = list(selectAttrs)\n",
    "    tempDict[\"joinAttrs\"] = list(joinAttrs)\n",
    "    tempDict[\"groupByAttrs\"] = list(groupByAttrs)\n",
    "    tempDict[\"orderByAttrs\"] = list(orderByAttrs)\n",
    "    tempDict[\"aggrAttrs\"] = list(aggrAttrs)\n",
    "    tempDict[\"whereAttrs\"] = list(whereAttrs)\n",
    "    return tempDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': ['select', 'distinct', 'kingdoms', '.', '*', 'from', 'taxonomic_units_types', 'join', 'kingdoms', 'on', 'taxonomic_units_types', '.', 'kingdom_id', '=', 'kingdoms', '.', 'id', 'where', 'taxonomic_units_types', '.', 'rank_name', '=', \"'\", 'order', \"'\"], 'entities': ['taxonomic_units_types', 'kingdoms'], 'selectAttrs': ['kingdoms.*'], 'joinAttrs': ['types.taxonomic_units_kingdom_id', 'kingdoms.id'], 'groupByAttrs': [], 'orderByAttrs': [], 'aggrAttrs': [], 'whereAttrs': [['taxonomic_units_types.rank_name', '=', 'value', 'None']]}\n"
     ]
    }
   ],
   "source": [
    "synonyms_dict = {}\n",
    "['select', 'distinct', 'kingdoms', '.', '*', 'from', 'taxonomic_units_types', 'join', 'kingdoms', 'on', 'taxonomic_units_types', '.', 'kingdom_id', '=', 'kingdoms', '.', 'id', 'where', 'taxonomic_units_types', '.', 'rank_name', \"!\",'=', \"'\", 'order', \"'\"]\n",
    "#test_query = [\"select\", \"name\", \"from\", \"track\", \"where\", \"milliseconds\", \"=\", \"(\", \"select\", \"max\", \"(\", \"milliseconds\", \")\", \"from\", \"track\", \")\"]\n",
    "#print(test_query)\n",
    "################ problem ################\n",
    "#test_query = [\"select\", \"t1\", \".\", \"name\", \"from\", \"member\", \"as\", \"t1\", \"join\", \"member_attendance\", \"as\", \"t2\", \"on\", \"t1\", \".\", \"member_id\", \"=\", \"t2\", \".\", \"member_id\", \"group\", \"by\", \"t2\", \".\", \"member_id\", \"order\", \"by\", \"count\", \"(\", \"*\", \")\", \"asc\", \"limit\", \"1\"]\n",
    "#################################################\n",
    "test_query = ['select', 'distinct', 'kingdoms', '.', '*', 'from', 'taxonomic_units_types', 'join', 'kingdoms', 'on', 'taxonomic_units_types', '.', 'kingdom_id', '=', 'kingdoms', '.', 'id', 'where', 'taxonomic_units_types', '.', 'rank_name', '=', \"'\", 'order', \"'\"]\n",
    "nested = separate_queries(test_query)\n",
    "for query in nested:\n",
    "    print(get_queries(query,test_query))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8919\n",
      "45942\n",
      "7247\n",
      "10228\n",
      "9493\n",
      "5183\n",
      "189\n",
      "214\n",
      "1134\n",
      "259\n",
      "99\n",
      "23\n",
      "284\n",
      "5183\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "############# this cell for datasets:  sparc and splash-master and cosql ###########\n",
    "import json\n",
    "import re\n",
    "synonyms_dict = {}\n",
    "datasets = [\"cosql\",\"greatSql\",\"nvBench\",\"sparc\",\"splash\",\"spider\",\"academic\",\"advising\",\"atis\",\"geography\",\"imdb\",\"restaurants\",\"scholar\",\"spider\",\"yelp\"]\n",
    "for ds in datasets:\n",
    "    keywordsDict = []\n",
    "    with open('/home/nihal/Desktop/newwhere/GP/notebooks/preparingDatasets/dataset_preprocessing/'+ds+'.json', 'r',encoding='UTF-8') as file:\n",
    "        data = json.load(file)\n",
    "        for query in data:\n",
    "            nestedQueries = []\n",
    "            query_tokens = query['query_toks']\n",
    "            separatedQueries = separate_queries(query_tokens)  \n",
    "            for currentQuery in separatedQueries:\n",
    "                concatQuery = ' '.join(currentQuery)\n",
    "                concatQuery = concatQuery.replace(\" . \",\".\")\n",
    "                tempDict = get_queries(currentQuery,concatQuery)\n",
    "                nestedQueries.append(tempDict)\n",
    "            outQuery = {\n",
    "                'query':query['query'],\n",
    "                'allQueries':nestedQueries\n",
    "            }\n",
    "            keywordsDict.append(outQuery)\n",
    "    print(len(keywordsDict))\n",
    "    outputFile = open(\"./finalOutputs/\"+ds+\".json\", \"w\")\n",
    "    json.dump(keywordsDict, outputFile)\n",
    "    outputFile.close()\n",
    "outputFile = open(\"./finalOutputs/synonyms.json\", \"w\")\n",
    "json.dump(synonyms_dict, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### splash-master data tokenization #####\n",
    "import re\n",
    "import json\n",
    "Dataset = []\n",
    "def read_data():\n",
    "    data = []\n",
    "    data_path = \"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/Splash-master/data\"\n",
    "    data_files = [data_path + '/' + f for f in os.listdir(data_path) if f.endswith('.json')]\n",
    "    for data_file in data_files:\n",
    "        with open(data_file) as f:\n",
    "            data_object = json.load(f)\n",
    "            queries = [data['gold_parse'] for data in data_object]\n",
    "            data.extend(queries)\n",
    "    return data\n",
    "data = read_data()\n",
    "\n",
    "for query in data:\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = query\n",
    "    tokensList = re.split('(\\W)', query)\n",
    "    tokensList = list(filter(None, tokensList))\n",
    "    tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "    convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "    tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "    Dataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/splash.json\", \"w\")\n",
    "json.dump(Dataset, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### sparc data tokenization #####\n",
    "import re\n",
    "import json\n",
    "Dataset = []\n",
    "def read_data():\n",
    "    data = []\n",
    "    data_path = \"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/sparc/sparc\"\n",
    "    data_files = [data_path + '/' + f for f in os.listdir(data_path) if f.endswith('.json')]\n",
    "    for data_file in data_files:\n",
    "        with open(data_file) as f:\n",
    "            data_object = json.load(f)\n",
    "            interactions = [data['interaction'] for data in data_object]\n",
    "            for interaction  in interactions:\n",
    "                for query in interaction:\n",
    "                    data.append(query['query'])\n",
    "    return data\n",
    "data = read_data()\n",
    "for query in data:\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = query\n",
    "    tokensList = re.split('(\\W)', query)\n",
    "    tokensList = list(filter(None, tokensList))\n",
    "    tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "    convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "    tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "    Dataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/sparc.json\", \"w\")\n",
    "json.dump(Dataset, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### nvBench data tokenization #####\n",
    "import re\n",
    "import json\n",
    "Dataset = []\n",
    "def read_data():\n",
    "    data = []\n",
    "    with open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/nvBench.json\") as f:\n",
    "        data_object = json.load(f)\n",
    "        queries = [data['vis_query']['data_part']['sql_part'] for data in data_object.values()]\n",
    "        data.extend(queries)\n",
    "    return data\n",
    "data = read_data()\n",
    "for query in data:\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = query\n",
    "    tokensList = re.split('(\\W)', query)\n",
    "    tokensList = list(filter(None, tokensList))\n",
    "    tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "    convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "    tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "    Dataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/nvBench.json\", \"w\")\n",
    "json.dump(Dataset, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cosql data tokenization #####\n",
    "import re\n",
    "import json\n",
    "Dataset = []\n",
    "def read_data():\n",
    "    data=[]\n",
    "    data_path = \"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/cosql_dataset/cosql_dataset/system_response_generation\"\n",
    "    data_files = [data_path + '/' + f for f in os.listdir(data_path) if f.endswith('.json')]\n",
    "    for data_file in data_files:\n",
    "        with open(data_file) as f:\n",
    "            data_object = json.load(f)\n",
    "            queries = [{\"query\":data['query'],\"query_toks\":data[\"query_toks\"]}for data in data_object]\n",
    "            data.extend(queries)\n",
    "    return data \n",
    "data = read_data()\n",
    "for query in data:\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = query[\"query\"]\n",
    "    tempDict[\"query_toks\"] = query['query_toks']\n",
    "    Dataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/cosql.json\", \"w\")\n",
    "json.dump(Dataset, outputFile)\n",
    "outputFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Topics data tokenization #####\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "datapath =\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/topics/\"\n",
    "data_files = os.listdir(datapath) \n",
    "for data_file in data_files:\n",
    "    Dataset = []\n",
    "    def read_data(datapth):\n",
    "        data=[]\n",
    "        with open(datapth) as f:\n",
    "            data_object = json.load(f)\n",
    "            queries = [data['sql'] for data in data_object]\n",
    "            for q in queries:\n",
    "                data.extend(q)\n",
    "        return data\n",
    "    data = read_data(datapath+data_file)\n",
    "    for query in data:\n",
    "        tempDict = {}\n",
    "        tempDict[\"query\"] = query\n",
    "        tokensList = re.split('(\\W)', query)\n",
    "        tokensList = list(filter(None, tokensList))\n",
    "        tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "        convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "        tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "        Dataset.append(tempDict)\n",
    "\n",
    "    outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/\"+data_file, \"w\")\n",
    "    json.dump(Dataset, outputFile)\n",
    "    outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GreatSql data tokenization #####\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "Dataset = []\n",
    "def read_data():\n",
    "    data=[]\n",
    "    datapath =\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/new/GreatSQL-master/data\"\n",
    "    data_files = os.listdir(datapath) \n",
    "    for data_file in data_files:\n",
    "        with open(datapath+\"/\"+data_file) as f:\n",
    "            data_object = json.load(f)\n",
    "            queries = [data['sql'] for data in data_object]\n",
    "            data.extend(queries)\n",
    "    return data\n",
    "data = read_data()\n",
    "for query in data:\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = query\n",
    "    tokensList = re.split('(\\W)', query)\n",
    "    tokensList = list(filter(None, tokensList))\n",
    "    tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "    convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "    tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "    Dataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nada/GP/GP/GP/notebooks/preparingDatasets/dataset_preprocessing/greatSql.json\", \"w\")\n",
    "json.dump(Dataset, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# old function #######################\n",
    "### my getqueries function ###\n",
    "stop_words = ['in','intersect','except','union','not',\"desc\",\"asc\",\"or\",\"and\",\"like\"]\n",
    "AggrFunctions = [\"count\",\"sum\",\"min\",\"max\",\"avg\"]\n",
    "tokensToSkip = [\",\",\"(\",\")\",\".\",\"=\",\"!\",\"by\",\"distinct\",\">\",\">=\",\"<\",\"<=\",\";\"]\n",
    "def get_queries(query,realQuery):\n",
    "    entities = []\n",
    "    selectAttrs = []\n",
    "    joinAttrs = []\n",
    "    groupByAttrs = []\n",
    "    orderByAttrs = []\n",
    "    aggrAttrs = []\n",
    "    whereAttrs = []\n",
    "    aliases = getEntitiesAliasesMapping(query)\n",
    "    i = 0\n",
    "    token = \"\"\n",
    "    wordType = \"\"\n",
    "    orderFunction = \"\"\n",
    "    whereCondition = \"\"\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "        if token == \"select\":\n",
    "            wordType = \"select\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token in stop_words:\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"limit\":\n",
    "            i += 2\n",
    "            continue\n",
    "        if token.find('\\\"') != -1 or token.find(\"'\") != -1 or token.find('\"') != -1:\n",
    "            i += 1\n",
    "            while i<len(query) and query[i].find('\\\"') == -1 and query[i].find(\"'\") == -1 and query[i].find('\"') == -1:\n",
    "                i += 1\n",
    "            i += 1\n",
    "            continue\n",
    "        if token.find(\"'\")!=-1 or token.find('\"')!=-1:\n",
    "            i+=1\n",
    "            continue\n",
    "        if token in tokensToSkip:\n",
    "            if token == \")\" and wordType == \"aggr\":\n",
    "                wordType = \"select\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == 'as':\n",
    "            i+=2\n",
    "            continue \n",
    "        if token == \"group\":\n",
    "            wordType = \"groupBy\"\n",
    "            i += 2\n",
    "            continue\n",
    "        if token == \"order\":\n",
    "            wordType = \"orderBy\"\n",
    "            if query[i+2] in AggrFunctions:\n",
    "                orderFunction = query[i+2]\n",
    "                i += 3\n",
    "            else: \n",
    "                orderFunction = \"\"\n",
    "                i += 2\n",
    "            continue\n",
    "        if token in AggrFunctions:\n",
    "            wordType = \"aggr\"\n",
    "            i+= 1\n",
    "            continue\n",
    "        if token == \"from\" or token == \"join\":\n",
    "            wordType = \"entity\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"where\":\n",
    "            wordType = \"whereClause\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"on\":\n",
    "            wordType = \"join\"\n",
    "            i += 1\n",
    "            continue\n",
    "        # check for numbers to skip\n",
    "        if re.match(r'[0-9]+',token):\n",
    "            i += 1\n",
    "            continue\n",
    "        if wordType in [\"select\",\"groupBy\",\"orderBy\",\"whereClause\",\"join\",\"aggr\"]:\n",
    "            if i+1 < len(query) and query[i+1] == \".\":\n",
    "                token = aliases.get(token,token)\n",
    "                token = token + \".\"+query[i+2]\n",
    "                i += 2\n",
    "            if wordType == \"whereClause\":\n",
    "                whereConditionsList = [\">\",\"<\",\"=\",\"!\"]\n",
    "                if i+1 < len(query):\n",
    "                    if query[i+1] in whereConditionsList:\n",
    "                        if i+2 < len(query) and query[i+2] in whereConditionsList:\n",
    "                            whereCondition = query[i+1] + query[i+2]\n",
    "                            i += 2\n",
    "                        else:\n",
    "                            whereCondition = query[i+1]\n",
    "                            i += 1\n",
    "            \n",
    "                \n",
    "        token = cleanTokens([token])[0]\n",
    "        sepToken = token.split(\".\")[-1]\n",
    "        token_syns = list(set(getSynonyms(sepToken)))\n",
    "        token_syns = cleanTokens(token_syns)\n",
    "        synonyms_dict[sepToken] = token_syns\n",
    "        if wordType == \"select\":\n",
    "            selectAttrs.append(token)\n",
    "        elif wordType == \"entity\":\n",
    "            entities.append(token)\n",
    "        elif wordType == \"join\":\n",
    "            joinAttrs.append(token)\n",
    "        elif wordType == \"groupBy\":\n",
    "            groupByAttrs.append(token)\n",
    "        elif wordType == \"orderBy\":\n",
    "            orderByAttrs.append((token, orderFunction))\n",
    "        elif wordType == \"aggr\":\n",
    "            aggrAttrs.append(token)\n",
    "        elif wordType == \"whereClause\":\n",
    "            whereAttrs.append((token,whereCondition))\n",
    "            whereCondition = \"\"\n",
    "        i += 1\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = realQuery\n",
    "    tempDict[\"entities\"] = list(entities)\n",
    "    tempDict[\"selectAttrs\"] = list(selectAttrs)\n",
    "    tempDict[\"joinAttrs\"] = list(joinAttrs)\n",
    "    tempDict[\"groupByAttrs\"] = list(groupByAttrs)\n",
    "    tempDict[\"orderByAttrs\"] = list(orderByAttrs)\n",
    "    tempDict[\"aggrAttrs\"] = list(aggrAttrs)\n",
    "    tempDict[\"whereAttrs\"] = list(whereAttrs)\n",
    "    return tempDict"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
