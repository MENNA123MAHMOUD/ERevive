{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractQueriesData (file):\n",
    "    inputFile = open(file)\n",
    " \n",
    "    data = json.load(inputFile)\n",
    "\n",
    "    queriesData = []\n",
    " \n",
    "    print(len(data))\n",
    "    \n",
    "    for i in data:\n",
    "        queriesDict = {}\n",
    "        for l in i[\"sql\"]:\n",
    "            queriesDict[\"query\"] = l\n",
    "            queriesDict[\"keywords\"] = extractKeywords(l)\n",
    "        queriesData.append(queriesDict)\n",
    " \n",
    "    inputFile.close()\n",
    "    return queriesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractKeywords (sqlQuery):\n",
    "    #### any word after . and any before as \n",
    "    keywords = set()\n",
    "    findAttr = re.compile('\\.\\w+')\n",
    "    #attributes.extend(findAttr.findall(sqlQuery))\n",
    "    tempAttr = findAttr.findall(sqlQuery)\n",
    "    findEntities = re.compile('\\w+ AS')\n",
    "    #entities.extend(findEntities.findall(sqlQuery))\n",
    "    tempEntities = findEntities.findall(sqlQuery)\n",
    "    for i in tempAttr:\n",
    "        keywords.add(i.split(\".\")[1])\n",
    "    for i in tempEntities:\n",
    "        keywords.add(i.split(\" AS\")[0])\n",
    "    \n",
    "    ###TO Do: we need to specify type of each keyword\n",
    "    return list(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n"
     ]
    }
   ],
   "source": [
    "########### main function ###############\n",
    "queriesData = extractQueriesData(\"./jsonFiles/scholar.json\")\n",
    "outputFile = open(\"./outputs/scholar.json\", \"w\")\n",
    "json.dump(queriesData, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' salary, gender ', 'Nihal', '   k,   jjj, kks ']\n"
     ]
    }
   ],
   "source": [
    "data = \"SELECT avg ( salary, gender )  FROM instructor (Nihal) ssgsgsg (   k,   jjj, kks )\"\n",
    "\n",
    "test = re.findall('\\(([^)]+)', data)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('*', 'Aggregation'), ('balance', 'Aggregation'), ('kdkd', 'Aggregation')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = \"SELECT sum ( balance, T.kdkd ), hdhdhddh , T1.name, (SELECT COUNT (*), NANA FROM HSHS ) FROM accounts AS T1 JOIN savings AS T2 ON T1.custid   =  T2.custid  where T1.name  =  \\\"O'mahony\\\" union SELECT sum ( T2.balance ) , T1.name FROM accounts AS T1 JOIN savings AS T2 ON T1.custid   =  T2.custid  where T1.name  =  \\\"Wang\\\" union SELECT sum ( T2.balance ) , T1.name FROM accounts AS T1 JOIN savings AS T2 ON T1.custid   =  T2.custid  where T1.name  =  \\\"Brown\\\"\"\n",
    "\n",
    "keywords = []\n",
    "\n",
    "selectPart = re.findall('SELECT (.*?) FROM', data)\n",
    "\n",
    "for i in selectPart:\n",
    "    i = re.sub('\\(SELECT',\"\",i)\n",
    "    aggrAttr = re.findall('\\(([^)]+)', i)\n",
    "    for attr in aggrAttr:\n",
    "        tempList = set()\n",
    "        tempAttr = attr.split(\",\") #array of attributes\n",
    "        for i in tempAttr:\n",
    "            dot = i.find(\".\")\n",
    "            if dot == -1:\n",
    "                tempList.add((i.strip(), \"Aggregation\"))\n",
    "            else:\n",
    "                tempList.add((i.strip().split(\".\")[1], \"Aggregation\"))\n",
    "        keywords.extend(tempList)\n",
    "print(list(set(keywords)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hdhdhddh', 'Select'), ('name', 'Select'), ('NANA', 'Select'), ('name', 'Select'), ('name', 'Select')]\n"
     ]
    }
   ],
   "source": [
    "### extract select attributes\n",
    "sql = \"SELECT sum ( balance, T.kdkd ), hdhdhddh , T1.name, (SELECT COUNT (*), NANA FROM HSHS ) FROM accounts AS T1 JOIN savings AS T2 ON T1.custid   =  T2.custid  where T1.name  =  \\\"O'mahony\\\" union SELECT sum ( T2.balance ) , T1.name FROM accounts AS T1 JOIN savings AS T2 ON T1.custid   =  T2.custid  where T1.name  =  \\\"Wang\\\" union SELECT sum ( T2.balance ) , T1.name FROM accounts AS T1 JOIN savings AS T2 ON T1.custid   =  T2.custid  where T1.name  =  \\\"Brown\\\"\"\n",
    "sql = re.sub('\\(([^)]+)',\"\",sql)\n",
    "aggrAttr = re.findall('SELECT (.*?) FROM', sql)\n",
    "keys = []\n",
    "for l in aggrAttr:\n",
    "    arr = l.split(\",\")\n",
    "    for i in arr:\n",
    "        if i.find(\")\") == -1:\n",
    "            if i.find(\".\") == -1:\n",
    "                keys.append((i.strip(),\"Select\"))\n",
    "            else:\n",
    "                keys.append(((i.strip().split(\".\")[1], \"Select\")))\n",
    "print(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Final Code starting from here #########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_queries(query):\n",
    "    queries=[]\n",
    "    runing_query=[]\n",
    "    i = 1\n",
    "    runing_query.append(query[0])\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "        if token == 'as':\n",
    "            i+=2\n",
    "            continue \n",
    "        if token == 'select':\n",
    "            prev_token = query[i-1]\n",
    "            nested_query = []\n",
    "            if prev_token == '(':\n",
    "                #WHILE TO GET BRACKET END\n",
    "                open_bracket = 1\n",
    "                while open_bracket >=1:\n",
    "                    token = query[i]\n",
    "                    i+=1\n",
    "                    if token == '(': open_bracket += 1\n",
    "                    elif token == ')': open_bracket -= 1\n",
    "                    if open_bracket == 0: break\n",
    "                    nested_query.append(token)\n",
    "            else:\n",
    "                nested_query = query[i:]\n",
    "                i = len(query)+1\n",
    "            nested_queries = separate_queries(nested_query)\n",
    "            queries.extend(nested_queries)\n",
    "        if i <= len(query): runing_query.append(token)\n",
    "        i+=1\n",
    "    queries.append(runing_query)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### my getqueries function ###\n",
    "stop_words = ['in','intersect','except','union','not',\"desc\",\"asc\",\"or\",\"and\",\"like\"]\n",
    "AggrFunctions = [\"count\",\"sum\",\"min\",\"max\",\"avg\"]\n",
    "tokensToSkip = [\",\",\"(\",\")\",\".\",\"=\",\"!\",\"by\",\"distinct\",\">\",\">=\",\"<\",\"<=\",\";\"]\n",
    "def get_queries(query,realQuery):\n",
    "    entities = []\n",
    "    selectAttrs = []\n",
    "    joinAttrs = []\n",
    "    groupByAttrs = []\n",
    "    orderByAttrs = []\n",
    "    aggrAttrs = []\n",
    "    whereAttrs = []\n",
    "    i = 0\n",
    "    token = \"\"\n",
    "    wordType = \"\"\n",
    "    orderFunction = \"\"\n",
    "    #isValue = False\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "        if token == \"select\":\n",
    "            wordType = \"select\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token in stop_words:\n",
    "            i += 1\n",
    "            continue\n",
    "        if i+1 < len(query) and query[i+1] == \".\": \n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"limit\":\n",
    "            i += 2\n",
    "            continue\n",
    "        # if token == \"=\" and wordType == \"whereClause\":\n",
    "        #     isValue = True\n",
    "        #     i += 1\n",
    "        #     continue\n",
    "        if token in tokensToSkip:\n",
    "            if token == \")\" and wordType == \"aggr\":\n",
    "                wordType = \"select\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"group\":\n",
    "            wordType = \"groupBy\"\n",
    "            i += 2\n",
    "            continue\n",
    "        if token == \"order\":\n",
    "            wordType = \"orderBy\"\n",
    "            if query[i+2] in AggrFunctions:\n",
    "                orderFunction = query[i+2]\n",
    "                i += 3\n",
    "            else: \n",
    "                orderFunction = \"\"\n",
    "                i += 2\n",
    "            continue\n",
    "        if token in AggrFunctions:\n",
    "            wordType = \"aggr\"\n",
    "            i+= 1\n",
    "            continue\n",
    "        if token == \"from\" or token == \"join\":\n",
    "            wordType = \"entity\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"where\":\n",
    "            wordType = \"whereClause\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"on\":\n",
    "            wordType = \"join\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token.find(\"'\") != -1 or token.find('\"') != -1:\n",
    "            #isValue = False\n",
    "            i += 1\n",
    "            continue\n",
    "        if token.find(\"\\\"\") != -1:\n",
    "            i += 1\n",
    "            while query[i].find(\"\\\"\") == -1:\n",
    "                i += 1\n",
    "            i += 1\n",
    "            #isValue = False\n",
    "            continue\n",
    "        # if isValue == True: #then it is number which you have to skip\n",
    "        #     isValue = False\n",
    "        #     i += 1\n",
    "        #     continue\n",
    "        # check for numbers to skip\n",
    "        if re.search('[0-9]+', token):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        if wordType == \"select\":\n",
    "            selectAttrs.append(token)\n",
    "        elif wordType == \"entity\":\n",
    "            entities.append(token)\n",
    "        elif wordType == \"join\":\n",
    "            joinAttrs.append(token)\n",
    "        elif wordType == \"groupBy\":\n",
    "            groupByAttrs.append(token)\n",
    "        elif wordType == \"orderBy\":\n",
    "            orderByAttrs.append((token, orderFunction))\n",
    "        elif wordType == \"aggr\":\n",
    "            aggrAttrs.append(token)\n",
    "        elif wordType == \"whereClause\":\n",
    "            whereAttrs.append(token)\n",
    "        i += 1\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = realQuery\n",
    "    tempDict[\"entities\"] = list(set(entities))\n",
    "    tempDict[\"selectAttrs\"] = list(set(selectAttrs))\n",
    "    tempDict[\"joinAttrs\"] = list(set(joinAttrs))\n",
    "    tempDict[\"groupByAttrs\"] = list(set(groupByAttrs))\n",
    "    tempDict[\"orderByAttrs\"] = list(set(orderByAttrs))\n",
    "    tempDict[\"aggrAttrs\"] = list(set(aggrAttrs))\n",
    "    tempDict[\"whereAttrs\"] = list(set(whereAttrs))\n",
    "    return tempDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entities': ['track'], 'selectAttrs': [], 'joinAttrs': [], 'groupByAttrs': [], 'orderByAttrs': [], 'aggrAttrs': ['milliseconds'], 'whereAttrs': []}, {'entities': ['track'], 'selectAttrs': ['name'], 'joinAttrs': [], 'groupByAttrs': [], 'orderByAttrs': [], 'aggrAttrs': [], 'whereAttrs': ['milliseconds']}]\n"
     ]
    }
   ],
   "source": [
    "### for testing ###\n",
    "data = [\n",
    "            \"select\",\n",
    "            \"name\",\n",
    "            \"from\",\n",
    "            \"track\",\n",
    "            \"where\",\n",
    "            \"milliseconds\",\n",
    "            \"=\",\n",
    "            \"(\",\n",
    "            \"select\",\n",
    "            \"max\",\n",
    "            \"(\",\n",
    "            \"milliseconds\",\n",
    "            \")\",\n",
    "            \"from\",\n",
    "            \"track\",\n",
    "            \")\"\n",
    "        ]\n",
    "\n",
    "queries = separate_queries(data)\n",
    "dataset = []\n",
    "for query in queries:\n",
    "    tempDict = get_queries(query)\n",
    "    dataset.append(tempDict)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('genre_id', 'join'), ('name', 'groupBy'), ('tracks', 'entity'), ('genres', 'entity'), ('id', 'join'), ('name', 'select')]\n"
     ]
    }
   ],
   "source": [
    "### for testing ###\n",
    "query_tokens =  [\n",
    "            \"select\",\n",
    "            \"t1\",\n",
    "            \".\",\n",
    "            \"name\",\n",
    "            \"from\",\n",
    "            \"genres\",\n",
    "            \"as\",\n",
    "            \"t1\",\n",
    "            \"join\",\n",
    "            \"tracks\",\n",
    "            \"as\",\n",
    "            \"t2\",\n",
    "            \"on\",\n",
    "            \"t1\",\n",
    "            \".\",\n",
    "            \"id\",\n",
    "            \"=\",\n",
    "            \"t2\",\n",
    "            \".\",\n",
    "            \"genre_id\",\n",
    "            \"group\",\n",
    "            \"by\",\n",
    "            \"t1\",\n",
    "            \".\",\n",
    "            \"name\"\n",
    "        ]\n",
    "\n",
    "queries = []\n",
    "current_queries = get_queries(query_tokens)      \n",
    "queries.extend(current_queries)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### update separate_queries to be suitable for sparc dataset ####\n",
    "def separate_queries_sparc(query):\n",
    "    queries=[]\n",
    "    runing_query=[]\n",
    "    i = 1\n",
    "    runing_query.append(query[0])\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "        if token == 'select':\n",
    "            prev_token = query[i-1]\n",
    "            nested_query = []\n",
    "            if prev_token == '(':\n",
    "                #WHILE TO GET BRACKET END\n",
    "                open_bracket = 1\n",
    "                while open_bracket >=1:\n",
    "                    token = query[i]\n",
    "                    i+=1\n",
    "                    if token == '(': open_bracket += 1\n",
    "                    elif token == ')': open_bracket -= 1\n",
    "                    if open_bracket == 0: break\n",
    "                    nested_query.append(token)\n",
    "            else:\n",
    "                nested_query = query[i:]\n",
    "                i = len(query)+1\n",
    "            nested_queries = separate_queries(nested_query)\n",
    "            queries.extend(nested_queries)\n",
    "        if i <= len(query): runing_query.append(token)\n",
    "        i+=1\n",
    "    queries.append(runing_query)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### update get_queries to be suitable for sparc dataset ####\n",
    "stop_words = ['in','intersect','except','union','not',\"desc\",\"asc\",\"or\",\"and\",\"like\"]\n",
    "AggrFunctions = [\"count\",\"sum\",\"min\",\"max\",\"avg\"]\n",
    "tokensToSkip = [\",\",\"(\",\")\",\".\",\"=\",\"!\",\"by\",\"distinct\",\">\",\">=\",\"<\",\"<=\",\";\"]\n",
    "def get_queries_sparc(query,realQuery):\n",
    "    entities = []\n",
    "    selectAttrs = []\n",
    "    joinAttrs = []\n",
    "    groupByAttrs = []\n",
    "    orderByAttrs = []\n",
    "    aggrAttrs = []\n",
    "    whereAttrs = []\n",
    "    i = 0\n",
    "    token = \"\"\n",
    "    wordType = \"\"\n",
    "    orderFunction = \"\"\n",
    "    while i < len(query):\n",
    "        token = query[i]\n",
    "        if token == \"select\":\n",
    "            wordType = \"select\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token in stop_words:\n",
    "            i += 1\n",
    "            continue\n",
    "        if i+1 < len(query) and query[i+1] == \".\": \n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"limit\":\n",
    "            i += 2\n",
    "            continue\n",
    "        stringSeparators = [\"\\\"\",\"'\",'\"']\n",
    "        if token in stringSeparators:\n",
    "            i += 1\n",
    "            while query[i] not in stringSeparators:\n",
    "                i += 1\n",
    "            i += 1\n",
    "            continue\n",
    "        if token in tokensToSkip:\n",
    "            if token == \")\" and wordType == \"aggr\":\n",
    "                wordType = \"select\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == 'as':\n",
    "            i+=2\n",
    "            continue \n",
    "        if token == \"group\":\n",
    "            wordType = \"groupBy\"\n",
    "            i += 2\n",
    "            continue\n",
    "        if token == \"order\":\n",
    "            wordType = \"orderBy\"\n",
    "            if query[i+2] in AggrFunctions:\n",
    "                orderFunction = query[i+2]\n",
    "                i += 3\n",
    "            else: \n",
    "                orderFunction = \"\"\n",
    "                i += 2\n",
    "            continue\n",
    "        if token in AggrFunctions:\n",
    "            wordType = \"aggr\"\n",
    "            i+= 1\n",
    "            continue\n",
    "        if token == \"from\" or token == \"join\":\n",
    "            wordType = \"entity\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"where\":\n",
    "            wordType = \"whereClause\"\n",
    "            i += 1\n",
    "            continue\n",
    "        if token == \"on\":\n",
    "            wordType = \"join\"\n",
    "            i += 1\n",
    "            continue\n",
    "        # check for numbers to skip\n",
    "        if re.search('[0-9]+', token):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        if wordType == \"select\":\n",
    "            selectAttrs.append(token)\n",
    "        elif wordType == \"entity\":\n",
    "            entities.append(token)\n",
    "        elif wordType == \"join\":\n",
    "            joinAttrs.append(token)\n",
    "        elif wordType == \"groupBy\":\n",
    "            groupByAttrs.append(token)\n",
    "        elif wordType == \"orderBy\":\n",
    "            orderByAttrs.append((token, orderFunction))\n",
    "        elif wordType == \"aggr\":\n",
    "            aggrAttrs.append(token)\n",
    "        elif wordType == \"whereClause\":\n",
    "            whereAttrs.append(token)\n",
    "        i += 1\n",
    "    tempDict = {}\n",
    "    tempDict[\"query\"] = realQuery\n",
    "    tempDict[\"entities\"] = list(set(entities))\n",
    "    tempDict[\"selectAttrs\"] = list(set(selectAttrs))\n",
    "    tempDict[\"joinAttrs\"] = list(set(joinAttrs))\n",
    "    tempDict[\"groupByAttrs\"] = list(set(groupByAttrs))\n",
    "    tempDict[\"orderByAttrs\"] = list(set(orderByAttrs))\n",
    "    tempDict[\"aggrAttrs\"] = list(set(aggrAttrs))\n",
    "    tempDict[\"whereAttrs\"] = list(set(whereAttrs))\n",
    "    return tempDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# this cell for datasets:  sparc and splash-master and cosql ###########\n",
    "import json\n",
    "import re\n",
    "keywordsDict = []\n",
    "with open('/home/nihal/Desktop/GP/dataset_preprocessing/splash-master/train.json', 'r',encoding='UTF-8') as file:\n",
    "    data = json.load(file)\n",
    "    for query in data:\n",
    "        nestedQueries = []\n",
    "        query_tokens = query['query_toks']\n",
    "        separatedQueries = separate_queries_sparc(query_tokens)  \n",
    "        for currentQuery in separatedQueries:\n",
    "            concatQuery = ' '.join(currentQuery)\n",
    "            concatQuery = concatQuery.replace(\" . \",\".\")\n",
    "            tempDict = get_queries_sparc(currentQuery,concatQuery)\n",
    "            nestedQueries.append(tempDict)\n",
    "        keywordsDict.append(nestedQueries)\n",
    "\n",
    "outputFile = open(\"./outputs/splash-master_train.json\", \"w\")\n",
    "json.dump(keywordsDict, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select count ( * ) from student where sex = \" m \" and stuid in ( select stuid from has_allergy as t1 join allergy_type as t2 on t1.allergy = t2.allergy where t2.allergytype = \" food \" )\n"
     ]
    }
   ],
   "source": [
    "################ testing joins for query tokens ###################\n",
    "query_toks = [\"select\", \"count\", \"(\", \"*\", \")\", \"from\", \"student\", \"where\", \"sex\", \"=\", \"\\\"\", \"m\", \"\\\"\", \"and\", \"stuid\", \"in\", \"(\", \"select\", \"stuid\", \"from\", \"has_allergy\", \"as\", \"t1\", \"join\", \"allergy_type\", \"as\", \"t2\", \"on\", \"t1\", \".\", \"allergy\", \"=\", \"t2\", \".\", \"allergy\", \"where\", \"t2\", \".\", \"allergytype\", \"=\", \"\\\"\", \"food\", \"\\\"\", \")\"]\n",
    "result = ' '.join(query_toks)\n",
    "result = result.replace(\" . \",\".\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### splash-master data tokenization #####\n",
    "import re\n",
    "import json\n",
    "sparcDataset = []\n",
    "with open (\"/home/nihal/Desktop/GP/dataset/Splash-master/data/dev.json\", 'r',encoding='UTF-8') as file:\n",
    "    data = json.load(file)\n",
    "    for query in data:\n",
    "        tempDict = {}\n",
    "        tempDict[\"query\"] = query[\"gold_parse\"]\n",
    "        tokensList = re.split('(\\W)', query[\"gold_parse\"])\n",
    "        tokensList = list(filter(None, tokensList))\n",
    "        tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "        convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "        tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "        sparcDataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nihal/Desktop/GP/dataset_preprocessing/splash-master/dev.json\", \"w\")\n",
    "json.dump(sparcDataset, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as\n",
      "SELECT count(*) FROM DEPARTMENT WHERE Division  =  \"AS\"\n",
      "[[{'query': 'SELECT CName FROM COURSE WHERE Days  =  \"MTW\"', 'entities': ['course'], 'selectAttrs': ['cname'], 'joinAttrs': [], 'groupByAttrs': [], 'orderByAttrs': [], 'aggrAttrs': [], 'whereAttrs': ['days']}, {'query': 'SELECT count(*) FROM DEPARTMENT WHERE Division  =  \"AS\"', 'entities': ['department'], 'selectAttrs': [], 'joinAttrs': [], 'groupByAttrs': [], 'orderByAttrs': [], 'aggrAttrs': ['*'], 'whereAttrs': ['division']}]]\n"
     ]
    }
   ],
   "source": [
    "###############test \n",
    "test = {\"query\": \"SELECT count(*) FROM DEPARTMENT WHERE Division  =  \\\"AS\\\"\", \"query_toks\": [\"select\", \"count\", \"(\", \"*\", \")\", \"from\", \"department\", \"where\", \"division\", \"=\", \"\\\"\", \"as\", \"\\\"\"]}\n",
    "\n",
    "import json\n",
    "import re\n",
    "keywordsDict = [] \n",
    "separatedQueries = separate_queries_sparc(test['query_toks'])  \n",
    "for currentQuery in separatedQueries:\n",
    "    tempDict = get_queries_sparc(currentQuery,test['query'])\n",
    "    nestedQueries.append(tempDict)\n",
    "keywordsDict.append(nestedQueries)\n",
    "\n",
    "print(keywordsDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### sparc data tokenization #####\n",
    "import re\n",
    "import json\n",
    "sparcDataset = []\n",
    "with open (\"/home/nihal/Desktop/GP/dataset/sparc/sparc/dev.json\", 'r',encoding='UTF-8') as file:\n",
    "    data = json.load(file)\n",
    "    for query in data:\n",
    "        tempDict = {}\n",
    "        tempDict[\"query\"] = query[\"final\"][\"query\"]\n",
    "        tokensList = re.split('(\\W)', query[\"final\"][\"query\"])\n",
    "        tokensList = list(filter(None, tokensList))\n",
    "        tokensList = list(filter(lambda x: x != ' ', tokensList))\n",
    "        convertedTokensList = (map(lambda x: x.lower(), tokensList))\n",
    "        tempDict[\"query_toks\"] = list(convertedTokensList)\n",
    "        sparcDataset.append(tempDict)\n",
    "\n",
    "outputFile = open(\"/home/nihal/Desktop/GP/dataset_preprocessing/sparcDataset/dev.json\", \"w\")\n",
    "json.dump(sparcDataset, outputFile)\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SELECT', ' ', 'name', ' ', 'FROM', ' ', 'department', ' ', 'GROUP', ' ', 'BY', ' ', 'departmentID', ' ', 'ORDER', ' ', 'BY', ' ', 'count', '(', 'departmentID', ')', '', ' ', 'DESC', ' ', 'LIMIT', ' ', '1', ';', '']\n",
      "---------------------------------------------\n",
      "['SELECT', 'name', 'FROM', 'department', 'GROUP', 'BY', 'departmentID', 'ORDER', 'BY', 'count', '(', 'departmentID', ')', 'DESC', 'LIMIT', '1', ';']\n"
     ]
    }
   ],
   "source": [
    "l = \"SELECT name FROM department GROUP BY departmentID ORDER BY count(departmentID) DESC LIMIT 1;\"\n",
    "print(re.split('(\\W)',l))\n",
    "results = re.split('(\\W)',l)\n",
    "\n",
    "print(\"---------------------------------------------\")\n",
    "results = list(filter(None, results))\n",
    "results = list(filter(lambda x: x != ' ', results))\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
